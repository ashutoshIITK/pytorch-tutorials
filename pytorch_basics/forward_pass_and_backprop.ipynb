{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train = True, download = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train = False, download = True, transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle= True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn #For OOP like setting\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__() # To run initialization method from the parent class (nn.Module) we are inheriting from\n",
    "        self.fc1 = nn.Linear(28*28, 64) # Input is flattened layer of 28x28\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x): # Input is x\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1090, 0.0886, 0.0998, 0.0971, 0.0942, 0.1044, 0.1127, 0.1036, 0.0946,\n",
       "         0.0959]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8, 2, 5, 7, 9, 1, 5, 9, 2, 3])\n",
      "tensor([4, 4, 1, 4, 6, 1, 1, 6, 6, 8])\n",
      "tensor([0, 6, 4, 2, 4, 0, 5, 0, 5, 5])\n",
      "tensor([6, 4, 4, 3, 5, 1, 8, 8, 5, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "EPOCHS = 4\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        # data is a batch of feature sets and labels\n",
    "        X, y = data\n",
    "        print(y)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above run has given us 4 batches with each batch containing 10 examples as in the trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4999, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5612, grad_fn=<NllLossBackward>)\n",
      "tensor(1.4645, grad_fn=<NllLossBackward>)\n",
      "tensor(1.5611, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "EPOCHS = 4\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        # data is a batch of feature sets and labels\n",
    "        X, y = data\n",
    "        net.zero_grad()  # We make the gradient zero after each batch otw it will keep on summing\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step() # For adjusting the weights\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.946\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # We don't want to calculate the gradients if we are just doing forward pass\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(f\"Accuracy: {round(correct/total, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADKpJREFUeJzt3X/oXfV9x/Hney5GklYwdGaZddElMhRh6fgSUzLFIXZ2FNQ/6po/2gzK0oLCWgqb+E/9ZyCy1vWP0DWdoQm01kKbmT9krYRCLNhgFKnWbGtm0zZLSFpSiN1Y/PXeH9+T8jV+v/fc3HvuPffr+/mAcO8959xz3hzy+n7uvZ9zPp/ITCTV8zt9FyCpH4ZfKsrwS0UZfqkowy8VZfilogy/VJThl4oy/FJRvzvNg10aK/MyVk/zkFIp/8f/8Fqei2G2HSv8EXEH8CXgEuBfMvOhQdtfxmpuitvGOaSkAQ7lgaG3Hfljf0RcAuwEPgzcAGyLiBtG3Z+k6RrnO/9m4GhmvpKZrwHfBO7spixJkzZO+K8CfrHg9fFm2dtExI6IOBwRh1/n3BiHk9SlccK/2I8K77g/ODN3ZeZcZs6tYOUYh5PUpXHCfxy4esHr9wMnxitH0rSME/5ngesi4tqIuBT4GLC/m7IkTdrIXX2Z+UZE3Ad8l/muvt2Z+ePOKpM0UWP182fmk8CTHdUiaYq8vFcqyvBLRRl+qSjDLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4paKmOnS3NE1HH9my5LqtW14ea98/ffj6getX7Ts01v6nwZZfKsrwS0UZfqkowy8VZfilogy/VJThl4qyn18z63/vvmng+qd3fqVlDy90V8wFNtwyeE7ajfsmdujO2PJLRRl+qSjDLxVl+KWiDL9UlOGXijL8UlFj9fNHxDHgVeBN4I3MnOuiKNXw3RNt/fCT66dv84mf3TJw/R8czClVMjldXOTz55n5qw72I2mK/NgvFTVu+BP4XkQ8FxE7uihI0nSM+7F/a2aeiIgrgaci4t8z8+DCDZo/CjsALmPVmIeT1JWxWv7MPNE8ngb2AZsX2WZXZs5l5twKVo5zOEkdGjn8EbE6It57/jnwIeClrgqTNFnjfOxfC+yLiPP7+UZm/lsnVUmauJHDn5mvAH/SYS1ahtruub/2745MqZJ3GtRXP+64+6uY/XH529jVJxVl+KWiDL9UlOGXijL8UlGGXyrKobs1lrauvL3rDw5cP44Nj3964PqNn/3hkuveDV1147Lll4oy/FJRhl8qyvBLRRl+qSjDLxVl+KWi7OfXQO3Da4+ubXjstttuN+5buh9f7Wz5paIMv1SU4ZeKMvxSUYZfKsrwS0UZfqko+/mLW/vM5b0d+9QHzw5c7z33k2XLLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFtfbzR8Ru4CPA6cy8sVm2BngcuAY4BtyTmb+eXJkaVdsU2nvXf2Wix7/53k8tuc5+/H4N0/J/DbjjgmX3Awcy8zrgQPNa0jLSGv7MPAicuWDxncCe5vke4K6O65I0YaN+51+bmScBmscruytJ0jRM/Nr+iNgB7AC4jFWTPpykIY3a8p+KiHUAzePppTbMzF2ZOZeZcytYOeLhJHVt1PDvB7Y3z7cDT3RTjqRpaQ1/RDwGPAP8cUQcj4hPAg8Bt0fET4Dbm9eSlpHIzKkd7PJYkzfFbVM7ntrv19+7/uCUKrl4447rv2pfvesIDuUBzuaZGGZbr/CTijL8UlGGXyrK8EtFGX6pKMMvFeXQ3e8Cg7rzZrkrr01r7TsHr7+ZAbcTF+wGvJAtv1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8VZT//MtD38NvL1dM7lz4vf7Fv0xQrmU22/FJRhl8qyvBLRRl+qSjDLxVl+KWiDL9UlP38M+DoI1sGrv+vv/rnKVVy8TY8/umB6zd+9ocj73s5n5flwJZfKsrwS0UZfqkowy8VZfilogy/VJThl4pq7eePiN3AR4DTmXljs+xB4G+AXzabPZCZT06qyOVulvur26bBPvXBswPXb2T0fnz1a5iW/2vAHYssfyQzNzX/DL60zLSGPzMPAmemUIukKRrnO/99EfGjiNgdEVd0VpGkqRg1/F8GNgCbgJPAF5baMCJ2RMThiDj8OudGPJykro0U/sw8lZlvZuZbwFeBzQO23ZWZc5k5t4KVo9YpqWMjhT8i1i14eTfwUjflSJqWYbr6HgNuBd4XEceBzwO3RsQmIIFjMGAuZEkzqTX8mbltkcWPTqCWZattXP1Zvu+8rR+/T1u3vDzW+weNNeD1CV7hJ5Vl+KWiDL9UlOGXijL8UlGGXyrKobuHNKg7b9BU0H1rHVp7wl1efZ63cYYNr8CWXyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeKsp9/SCduiYntu2347DZ71x9ccl3bbbGnxjpy++3Mk+zLbz9vs3u78iyw5ZeKMvxSUYZfKsrwS0UZfqkowy8VZfilouznH9Ikh9/+6cPXj7eDnUv38w+6BgDgE88M7itvez+80LJ+dONOH67BbPmlogy/VJThl4oy/FJRhl8qyvBLRRl+qajWfv6IuBrYC/w+8BawKzO/FBFrgMeBa4BjwD2Z+evJlfru1ee4/+39+JPTOqeA4+5P1DAt/xvA5zLzemALcG9E3ADcDxzIzOuAA81rSctEa/gz82RmPt88fxU4AlwF3AnsaTbbA9w1qSIlde+ivvNHxDXAB4BDwNrMPAnzfyCAK7suTtLkDB3+iHgP8G3gM5k59EXVEbEjIg5HxOHXOTdKjZImYKjwR8QK5oP/9cz8TrP4VESsa9avA04v9t7M3JWZc5k5t4KVXdQsqQOt4Y+IAB4FjmTmFxes2g9sb55vB57ovjxJkzLMLb1bgY8DL0bE+fs3HwAeAr4VEZ8Efg58dDIlajkb1J1nV16/WsOfmT8Alhq0/rZuy5E0LV7hJxVl+KWiDL9UlOGXijL8UlGGXyrKobuHdPO9n1py3aRvyZ3kra9tU2yv2ndo5H0DbMS+/Fllyy8VZfilogy/VJThl4oy/FJRhl8qyvBLRUVmTu1gl8eavCm8C1ialEN5gLN5Zqlb8N/Gll8qyvBLRRl+qSjDLxVl+KWiDL9UlOGXijL8UlGGXyrK8EtFGX6pKMMvFWX4paIMv1SU4ZeKag1/RFwdEd+PiCMR8eOI+Ntm+YMR8d8R8ULz7y8nX66krgwzaccbwOcy8/mIeC/wXEQ81ax7JDP/cXLlSZqU1vBn5kngZPP81Yg4Alw16cIkTdZFfeePiGuADwDn53C6LyJ+FBG7I+KKJd6zIyIOR8Th1zk3VrGSujN0+CPiPcC3gc9k5lngy8AGYBPznwy+sNj7MnNXZs5l5twKVnZQsqQuDBX+iFjBfPC/npnfAcjMU5n5Zma+BXwV2Dy5MiV1bZhf+wN4FDiSmV9csHzdgs3uBl7qvjxJkzLMr/1bgY8DL0bEC82yB4BtEbEJSOAYsPQc1pJmzjC/9v8AWGwc8Ce7L0fStHiFn1SU4ZeKMvxSUYZfKsrwS0UZfqkowy8VZfilogy/VJThl4oy/FJRhl8qyvBLRRl+qajIzOkdLOKXwM8WLHof8KupFXBxZrW2Wa0LrG1UXda2PjN/b5gNpxr+dxw84nBmzvVWwACzWtus1gXWNqq+avNjv1SU4ZeK6jv8u3o+/iCzWtus1gXWNqpeauv1O7+k/vTd8kvqSS/hj4g7IuI/IuJoRNzfRw1LiYhjEfFiM/Pw4Z5r2R0RpyPipQXL1kTEUxHxk+Zx0WnSeqptJmZuHjCzdK/nbtZmvJ76x/6IuAT4T+B24DjwLLAtM1+eaiFLiIhjwFxm9t4nHBG3AL8B9mbmjc2yh4EzmflQ84fzisz8+xmp7UHgN33P3NxMKLNu4czSwF3AX9PjuRtQ1z30cN76aPk3A0cz85XMfA34JnBnD3XMvMw8CJy5YPGdwJ7m+R7m//NM3RK1zYTMPJmZzzfPXwXOzyzd67kbUFcv+gj/VcAvFrw+zmxN+Z3A9yLiuYjY0Xcxi1jbTJt+fvr0K3uu50KtMzdP0wUzS8/MuRtlxuuu9RH+xWb/maUuh62Z+afAh4F7m4+3Gs5QMzdPyyIzS8+EUWe87lof4T8OXL3g9fuBEz3UsajMPNE8ngb2MXuzD586P0lq83i653p+a5Zmbl5sZmlm4NzN0ozXfYT/WeC6iLg2Ii4FPgbs76GOd4iI1c0PMUTEauBDzN7sw/uB7c3z7cATPdbyNrMyc/NSM0vT87mbtRmve7nIp+nK+CfgEmB3Zv7D1ItYRET8EfOtPcxPYvqNPmuLiMeAW5m/6+sU8HngX4FvAX8I/Bz4aGZO/Ye3JWq7lfmPrr+dufn8d+wp1/ZnwNPAi8BbzeIHmP9+3du5G1DXNno4b17hJxXlFX5SUYZfKsrwS0UZfqkowy8VZfilogy/VJThl4r6fyg8r1btwoPXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[2].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6, grad_fn=<NotImplemented>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[2].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of the notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
